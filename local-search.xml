<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>如何理解RNN中的梯度消失</title>
    <link href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%A1%B0%E5%87%8F/"/>
    <url>/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%A1%B0%E5%87%8F/</url>
    
    <content type="html"><![CDATA[<p>本文阐明了RNN中的梯度消失问题的含义，并根据反向传播公式深入分析了RNN中梯度消失的原因，最后说明了梯度消失和长距离依赖之间的关系。</p><span id="more"></span><h2 id="dnn中的梯度消失">1. DNN中的梯度消失</h2><p>在分析RNN梯度消失的原因之前，我们先回顾一下DNN（多层感知机）中梯度消失是如何产生的。</p><p>考虑一个具有n个隐藏层的DNN，前向传播时第<spanclass="math inline">\(i\)</span>个隐层的输出作为第<spanclass="math inline">\(i+1\)</span>个隐层的输入： <spanclass="math display">\[\begin{array}{l}\mathbf{H}^{(1)}&amp;=\sigma\left(\mathbf{W}^{(1)} \mathbf{X}+\mathbf{b}^{(1)}\right) \\\dots\\\mathbf{H}^{(n)}&amp;=\sigma\left(\mathbf{W}^{(n)} \mathbf{H}^{(n-1)}+\mathbf{b}^{(n)}\right) \\\mathbf{O}&amp;= \mathbf{W}^{(n+1)} \mathbf{H}^{(n)}+\mathbf{b}^{(n+1)}.\end{array}\tag{1}\]</span> 损失函数为： <span class="math display">\[L =  l(\mathbf{O},\mathbf{Y}) \tag{2}\]</span> 反向传播时根据链式法则有： <span class="math display">\[\begin{aligned}\frac{\partial L}{\partial \mathbf{W}^{(i)}} &amp;= \frac{\partiall}{\partial \mathbf{O}} \frac{\partial \mathbf{O}}{\partial\mathbf{H}^{(n)}}\frac{\partial \mathbf{H}^{(n)}}{\partial \mathbf{H}^{(n-1)}}\dots\frac{\partial \mathbf{H}^{(i)}}{\partial \mathbf{W}^{(i)}} \\&amp; =\frac{\partial l}{\partial \mathbf{O}}\frac{\partial \mathbf{O}}{\partial \mathbf{H}^{(n)}}\left(\prod_{j=i}^{n-1}  \frac{\partial \mathbf{H}^{(j+1)}}{\partial \mathbf{H}^{(j)}}\right)\frac{\partial \mathbf{H}^{(i)}}{\partial \mathbf{W}^{(i)}} \\\end{aligned}\tag{3}\]</span></p><p><span class="math inline">\(\frac{\partial l}{\partial\mathbf{O}}\)</span>、<span class="math inline">\(\frac{\partial\mathbf{O}}{\partial \mathbf{H}^{(n)}}\)</span>和<spanclass="math inline">\(\frac{\partial \mathbf{H}^{(i)}}{\partial\mathbf{W}^{(i)}}\)</span>都很容易计算，我们重点关注<spanclass="math inline">\(\prod_{j=i}^{n-1}\frac{\partial\mathbf{H}^{(j+1)}}{\partial\mathbf{H}^{(j)}}\)</span>这一部分，对于连乘中的任意一项有： <spanclass="math display">\[\frac{\partial \mathbf{H}^{(j+1)}}{\partial \mathbf{H}^{(j)}} ={\mathbf{W}^{(j)}}^\top \odot {\sigma}&#39;(\mathbf{W}^{(j)}\mathbf{H}^{(j-1)} +\mathbf{b}^{(j)})\tag{4}\]</span> 其中<span class="math inline">\({\sigma}&#39;(\mathbf{W}^{(j)}\mathbf{H}^{(j-1)}+\mathbf{b}^{(j)})\)</span>是激活函数的导数，若选用<spanclass="math inline">\(\tanh\)</span>作为激活函数，则<spanclass="math inline">\({\sigma}&#39; \in (0,1]\)</span>；若选用<spanclass="math inline">\(\mathrm{sigmoid}\)</span>作为激活函数，则<spanclass="math inline">\({\sigma}&#39; \in(0,0.25]\)</span>，可见二者的导数值都不大于1。</p><p><img src="https://pic.qinyu.space/image/image-20231222154756936.png" alt="image-20231222154756936" style="zoom:50%;" /></p><p>对于<span class="math inline">\(\prod_{k=i}^{n-1}\frac{\partial\mathbf{H}^{(k+1)}}{\partial \mathbf{H}^{(k)}} = \prod_{k=i}^{n-1}{\mathbf{W}^{(k)}}^\top\odot{\sigma}&#39;(\mathbf{W}^{(k)}\mathbf{H}^{(k-1)}+\mathbf{b}^{(k)})\)</span>，当<spanclass="math inline">\(i\)</span>较小时，也就是反向传播到靠近输入层时，将会出现多个参数矩阵<spanclass="math inline">\({\mathbf{W}^{(j)}}^\top\)</span>和多个<spanclass="math inline">\({\sigma}&#39;\)</span>连乘，多个矩阵连乘可能会导致梯度以指数速度增大或减小，而多<spanclass="math inline">\({\sigma}&#39;\)</span>个连乘则会导致梯度以指数速度减小，因为<spanclass="math inline">\({\sigma}&#39;\)</span>恒小于1。当二者综合表现为减小的趋势时，就有可能发生梯度消失。</p><p>总结一下就是在反向传播的过程中，用于训练参数的目标函数值，也就是<code>loss</code>，会随着距离的增加会呈指数级减小，因此在对<strong>靠近输入层的参数</strong>求偏导得到的梯度值也会呈指数级减小。这个问题的直接后果就是靠近输入层的一些层的参数很少会更新，或者说更新幅度很小。</p><h2 id="rnn中的梯度消失">2. RNN中的梯度消失</h2><p>现在我们以同样的方式来推导RNN中的反向传播公式。不过需要注意的是，在DNN中每一个隐层有单独的一个参数矩阵，每层的参数矩阵不共享，在RNN中，我们将RNN中每一个时间步视为一层，每一层的参数是共享的，因此我们可以将RNN展开看成<strong>共享参数</strong>的多层感知机。</p><p><img src="https://pic.qinyu.space/image/image-20231219202855121.png" alt="image-20231219202855121" style="zoom: 50%;" /></p><p>为了保持简单，我们考虑一个没有偏置参数的循环神经网络，有以下前向传播公式：<span class="math display">\[\begin{aligned}\mathbf{h}_t &amp;= \sigma \left( \mathbf{W}_{hx} \mathbf{x}_t +\mathbf{W}_{hh} \mathbf{h}_{t-1} \right) ,\\\mathbf{o}_t &amp;= \mathbf{W}_{qh} \mathbf{h}_{t},\\\end{aligned}\tag{5}\]</span>与DNN只在最后输出层才会输出值用于误差计算不同，RNN在每一个时间步都会有输出，并且输出的值最终都会参与误差计算。用<spanclass="math inline">\(l_t(\mathbf{o}_t, y_t)\)</span>表示时间步<spanclass="math inline">\(t\)</span>处的损失函数，反向传播时，需要分别计算<spanclass="math inline">\(\frac{\partial l_t}{\partial \bfW_{xh}}\)</span>、<span class="math inline">\(\frac{\partiall_t}{\partial \bf W_{hh}}\)</span>、<spanclass="math inline">\(\frac{\partial l_t}{\partial \bfW_{qh}}\)</span></p><p>其中<span class="math inline">\(\frac{\partial l_t}{\partial \bfW_{qh}}\)</span>很好计算： <span class="math display">\[\frac{\partial l_t}{\partial \mathbf{W}_{qh}}=  \frac{\partial l_t}{\partial \mathbf{o}_t} \frac{\partial\mathbf{o}_t}{\partial \mathbf{W}_{qh}}=  \frac{\partial l_t}{\partial \mathbf{o}_t} \mathbf{h}_t^\top\tag{6}\]</span> 但是<span class="math inline">\(\frac{\partial l_t}{\partial\mathbf W_{hx}}\)</span>、<span class="math inline">\(\frac{\partiall_t}{\partial \mathbf W_{hh}}\)</span>的计算要复杂的多，目标函数<spanclass="math inline">\(l_t\)</span>通过<spanclass="math inline">\(\mathbf{o}_{t}\)</span>，<spanclass="math inline">\(\mathbf{o}_{t}\)</span>通过<spanclass="math inline">\(\mathbf{h}_{t}\)</span>直接或通过隐状态<spanclass="math inline">\(\mathbf{h}_1, \ldots,\mathbf{h}_{t-1}\)</span>间接依赖于隐藏层中的模型参数<spanclass="math inline">\(\mathbf{W}_{hx}\)</span>和<spanclass="math inline">\(\mathbf{W}_{hh}\)</span>，也就是说隐状态<spanclass="math inline">\(\mathbf{h}_1, \ldots,\mathbf{h}_{t}\)</span>均参与了<spanclass="math inline">\(l_t\)</span>对<spanclass="math inline">\(\mathbf{W}_{hx}\)</span>和<spanclass="math inline">\(\mathbf{W}_{hh}\)</span>梯度的计算，整体的梯度是时间步从0-t的梯度之和。</p><p><img src="https://pic.qinyu.space/image/image-20231219230231191.png" alt="image-20231219230231191" style="zoom: 50%;" /></p><p>根据链式法则可以得到： <span class="math display">\[\begin{aligned}\frac{\partial l_t}{\partial \mathbf{W}_{hx}}&amp;= \sum_{i=1}^t \frac{\partial l_t}{\partial \mathbf{o}_t}\frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_{t}} \frac{\partial\mathbf{h}_t}{\partial \mathbf{h}_{i}}\frac{\partial\mathbf{h}_i}{\partial \mathbf{W}_{hx}},\\\frac{\partial l_t}{\partial \mathbf{W}_{hh}}&amp;= \sum_{i=1}^t \frac{\partial l_t}{\partial \mathbf{o}_t}\frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_{t}} \frac{\partial\mathbf{h}_t}{\partial \mathbf{h}_{i}}\frac{\partial\mathbf{h}_i}{\partial \mathbf{W}_{hh}}\end{aligned}\tag{7}\]</span> 其中<span class="math inline">\(\frac{\partial\mathbf{h}_t}{\partial\mathbf{h}_{i}}\)</span>计算需要再次用到链式法则： <spanclass="math display">\[\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{i}} = \frac{\partial\mathbf{h}_t}{\partial \mathbf{h}_{t-1}} \frac{\partial\mathbf{h}_{t-1}}{\partial \mathbf{h}_{t-2}} \dots \frac{\partial\mathbf{h}_{i+1}}{\partial \mathbf{h}_{i}}= \prod_{k=i}^{t-1} \frac{\partial \mathbf{h}_{k+1}} {\partial\mathbf{h}_{k}}\tag{8}\]</span> 对于连乘中任意一项有： <span class="math display">\[\frac{\partial \mathbf{h}_{k+1}}{\partial \mathbf{h}_{k}} =\mathbf{W}_{hh}^{\top}\odot \sigma&#39; \left( \mathbf{W}_{hx} \mathbf{x}_k + \mathbf{W}_{hh}\mathbf{h}_{k-1} \right)\tag{9}\]</span> 记<span class="math inline">\(\sigma_{k}&#39; =\sigma&#39;\left( \mathbf{W}_{hx} \mathbf{x}_k + \mathbf{W}_{hh} \mathbf{h}_{k-1}\right)\)</span>，则有： <span class="math display">\[\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{i}}= \prod_{k=i}^{t-1} \frac{\partial \mathbf{h}_{k+1}} {\partial\mathbf{h}_{k}}= \prod_{k=i}^{t-1} \mathbf{W}_{hh}^{\top} \odot \sigma_{k}&#39;\tag{10}\]</span> 我们发现式<spanclass="math inline">\((10)\)</span>和DNN中式<spanclass="math inline">\((4)\)</span>出现了同样的问题：当<spanclass="math inline">\(i\)</span>很小时，矩阵高次幂带来的不稳定性和激活函数的导数连乘带来的指数速度的衰减有可能导致偏导数<spanclass="math inline">\(\frac{\partial \mathbf{h}_t}{\partial\mathbf{h}_{i}}\)</span>的爆炸或消失。但是要由<spanclass="math inline">\(\frac{\partial\mathbf{h}_t}{\partial\mathbf{h}_{i}}\)</span>消失推出梯度消失还存在一个问题：<spanclass="math inline">\(\frac{\partial \mathbf{h}_t}{\partial\mathbf{h}_{i}}\)</span>很小只能代表<spanclass="math inline">\(\frac{\partial l_t}{\partial\mathbf{W}_{hx}}\)</span>计算式<spanclass="math inline">\((8)\)</span>中某一项很小，<spanclass="math inline">\(\frac{\partial l_t}{\partial\mathbf{W}_{hx}}\)</span>整体由于是各个项的累加，因此基本不会因为某一项的消失而导致总体的消失，那么何来梯度消失这一说法呢？这是因为梯度消失在RNN相对于DNN有着不同的含义。</p><p>我们不妨再回顾DNN和RNN模型之间的区别：</p><ul><li><p>在DNN中，每层有单独的参数，反向传播时，目标函数到任意一层之间的参数有且仅有1条路径，因此在式<spanclass="math inline">\((3)\)</span>中没有出现多条路径梯度的累加，因此唯一1条路径上发生梯度消失就会使整体的梯度消失。</p></li><li><p>在RNN中，每一个时间步共享参数矩阵<spanclass="math inline">\(\mathbf{W}_{hx}\)</span>和<spanclass="math inline">\(\mathbf{W}_{hh}\)</span>，目标函数到参数<spanclass="math inline">\(\mathbf{W}_{hx}\)</span>和<spanclass="math inline">\(\mathbf{W}_{hh}\)</span>有多条路径。举个例子，求<spanclass="math inline">\(l_3\)</span>对<spanclass="math inline">\(\mathbf{W}_{hx}\)</span>的梯度，由图可以看出一共有3条路径，分别用红、绿、蓝标出。反向传播时，对于参数从源节点到目标函数的每条路径，我们都需要计算沿着该路径的梯度，并将这些梯度相加以得到该目标函数对该参数的总梯度。</p><p><img src="https://pic.qinyu.space/image/image-20231222112644910.png" alt="image-20231222112644910" style="zoom:50%;" /></p></li></ul><p>对于<span class="math inline">\(\frac{\partial l_t}{\partial\mathbf{W}_{hx}} = \sum_{i=1}^t \frac{\partial l_t}{\partial\mathbf{o}_t} \frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_{t}}\frac{\partial \mathbf{h}_t}{\partial \mathbf{h}_{i}}\frac{\partial\mathbf{h}_i}{\partial \mathbf{W}_{hx}}\)</span>中的任意一项梯度<spanclass="math inline">\(\frac{\partial l_t}{\partial \mathbf{o}_t}\frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_{t}} \frac{\partial\mathbf{h}_t}{\partial \mathbf{h}_{i}}\frac{\partial\mathbf{h}_i}{\partial \mathbf{W}_{hx}}\)</span>，其实就代表<spanclass="math inline">\(l_t\)</span>到<spanclass="math inline">\(\mathbf{W}_{hx}\)</span>的一条路径，只不过这些路径有长有短。路径越长，对应的<spanclass="math inline">\(\frac{\partial \mathbf{h}_t}{\partial\mathbf{h}_{i}}\)</span>产生的乘法链就越长，由式<spanclass="math inline">\((10)\)</span>可以看出该路径的梯度就会消失，但是只要其他路径上还存在梯度就不会导致整体梯度的消失。<strong>因此RNN中总的梯度是不会消失的，即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的梯度不会消失，所有梯度之和便不会消失</strong>。</p><p><strong>那么RNN中远距离的梯度消失与RNN捕捉不到长距离依赖有什么关系呢？</strong></p><p>我们仍从表达式来分析，将梯度<spanclass="math inline">\(\frac{\partial l_t}{\partial \mathbf{o}_t}\frac{\partial \mathbf{o}_t}{\partial \mathbf{h}_{t}} \frac{\partial\mathbf{h}_t}{\partial \mathbf{h}_{i}}\frac{\partial\mathbf{h}_i}{\partial \mathbf{W}_{hx}}\)</span>合并一部分写成<spanclass="math inline">\(\frac{\partial l_t}{\partial \mathbf{h}_i}\frac{\partial \mathbf{h}_i}{\partial\mathbf{W}_{hx}}\)</span>，从数学角度来看，<spanclass="math inline">\(\frac{\partial l_t}{\partial\mathbf{h}_i}\)</span>描述的是损失函数<spanclass="math inline">\(l_t\)</span>对隐状态 <spanclass="math inline">\(\mathbf{h}_i\)</span>的偏导数，它反映了 <spanclass="math inline">\(\mathbf{h}_i\)</span> 的微小变化如何影响 <spanclass="math inline">\(l_t\)</span>。在RNN中，长期依赖可以被理解为当前的输出（或者损失函数）对过去较早时间步的隐状态的依赖，这种依赖可以通过<span class="math inline">\(\frac{\partial l_t}{\partial\mathbf{h}_i}\)</span> 来捕捉。</p><p>对于梯度 <span class="math inline">\(\frac{\partial l_t}{\partial\mathbf{h}_i} \frac{\partial \mathbf{h}_i}{\partial\mathbf{W}_{hx}}\)</span>，损失函数<spanclass="math inline">\(l_t\)</span>对隐状态 <spanclass="math inline">\(\mathbf{h}_i\)</span>的依赖信息作为梯度构成的一部分，通过作用于梯度来影响参数的更新。如果依赖信息<span class="math inline">\(\frac{\partial l_t}{\partial\mathbf{h}_i}\)</span> 很小，会导致梯度<spanclass="math inline">\(\frac{\partial l_t}{\partial \mathbf{h}_i}\frac{\partial \mathbf{h}_i}{\partial\mathbf{W}_{hx}}\)</span>也很小（梯度消失），那么这些依赖信息对参数更新的影响也会很小，网络可能无法有效地学习到这些长距离的依赖信息，也就是捕捉不到长距离依赖。</p><p><strong>所以RNN中梯度消失的真正含义是：在梯度更新时，由于长距离的依赖关系较弱，近距离依赖关系对整体梯度构成的影响更大，梯度被近距离的梯度主导，网络更倾向于利用近距离的梯度来更新参数，导致难以学到长距离的依赖关系。</strong></p><p>最后举一个语言建模的例子来充分理解长期依赖的含义：<img src="https://pic.qinyu.space/image/image-20231219200118619.png" style="zoom:50%;" /></p><p>观察上述两个例子，会发现句子开头的“Dog”这个词影响了位于最后的单词“has”，如果我们将单数词改为复数词“Dogs”，则应该将单词“has”改为"have"。对于上述例句，中间的句子可能会很长，位于末尾的单词受到几乎位于该句子开头的单词的影响，这就是我们所说的“长期依赖”。</p><p>要想捕捉这种长期依赖，可以选择更复杂的LSTM和GRU模型，我们将在下一节分析LSTM和GRU是如何缓解梯度消失的。</p><p><strong>【参考】</strong></p><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><ahref="https://kexue.fm/archives/7888">https://kexue.fm/archives/7888</a><a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><ahref="https://zhuanlan.zhihu.com/p/76772734">https://zhuanlan.zhihu.com/p/76772734</a><a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><ahref="https://zhuanlan.zhihu.com/p/109519044">https://zhuanlan.zhihu.com/p/109519044</a><a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RNN</tag>
      
      <tag>梯度消失</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客搭建（一）| 利用cloudflare加速github博客访问</title>
    <link href="/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/%E5%88%A9%E7%94%A8cloudflare%E5%8A%A0%E9%80%9Fgithub%E4%B8%BB%E9%A1%B5%E8%AE%BF%E9%97%AE/"/>
    <url>/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/%E5%88%A9%E7%94%A8cloudflare%E5%8A%A0%E9%80%9Fgithub%E4%B8%BB%E9%A1%B5%E8%AE%BF%E9%97%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="前言">1. 前言</h2><p>对于国内大陆用户而言，想要直接访问用 github托管的个人博客不是件容易的事，为了解决网络不可达问题，可以采用 <ahref="https://www.cloudflare.com/zh-hans-cn/learning/cdn/what-is-a-cdn/">CDN(contentdeliverynetwork)</a>将网页内容分发到全球各地的服务器上，同时还能缩短网站加载时间。</p><figure><img src="https://pic.qinyu.space/image/image-20231204215539220.png"alt="image-20231204215539220" /><figcaption aria-hidden="true">image-20231204215539220</figcaption></figure><p>但是大部分国内的CDN服务都是收费的，所以就把目光转向了cloudflare。cloudflare 作为全球最大的网络服务提供商，提供免费的cdn服务，虽然 cdn节点都在国外，但还是比直接访问github.io要快的多，不过免费版请求次数限制有10w次的限制，但对于我们博客而言是绰绰有余了，下面介绍配置过程。</p><h2 id="准备">2. 准备</h2><p>使用 cloudflare 的 cdn服务需要我们拥有一个可配置的域名，所以需要先购买一个域名，本人是在腾讯云上购买的<code>.space</code>的后缀的域名，10年价格也只要一百多，还是很便宜的，购买域名的教程就跳过了。</p><h2 id="教程">3. 教程</h2><p>注：教程中的<code>qinyu.me</code>和<code>qinyu.space</code>都是本人域名，所以在以下内容中可以视为同一个。</p><h3 id="配置-cloudflare">3.1 配置 cloudflare</h3><ol type="1"><li><p>进入<ahref="https://www.cloudflare-cn.com/">https://www.cloudflare-cn.com/</a>，注册账号并登录</p></li><li><p>在左侧栏中进入<code>网站</code>一栏，点击右方<code>添加站点</code></p><figure><img src="https://pic.qinyu.space/image/image-20231204222749831.png"alt="image-20231204222749831" /><figcaption aria-hidden="true">image-20231204222749831</figcaption></figure></li><li><p>输入自己的域名，<strong>注意不要带<code>www</code>或者<code>https</code></strong>，比如我的就直接填写<code>qinyu.space</code></p><figure><img src="https://pic.qinyu.space/image/image-20231204223059530.png"alt="image-20231204223059530" /><figcaption aria-hidden="true">image-20231204223059530</figcaption></figure></li><li><p>选择套餐，<code>free</code>即可</p><figure><img src="https://pic.qinyu.space/image/image-20231204223242693.png"alt="image-20231204223242693" /><figcaption aria-hidden="true">image-20231204223242693</figcaption></figure></li><li><p>点击继续后 cloudflare 会自动扫描域名的 dns记录，如果是刚刚创建的域名，可能扫描的结果为空。截图中的几条记录可以不用管</p><figure><img src="https://pic.qinyu.space/image/image-20231204223942549.png"alt="image-20231204223942549" /><figcaption aria-hidden="true">image-20231204223942549</figcaption></figure></li><li><p><strong>这一步很重要，点击添加记录，按照如下方式添加类型为A，名称为@，IPv4地址为<code>185.199.108.153</code></strong></p><figure><img src="https://pic.qinyu.space/image/image-20231204224450727.png"alt="image-20231204224450727" /><figcaption aria-hidden="true">image-20231204224450727</figcaption></figure><p><strong>按照上述方式再添加以下三条记录，类型和名称和上述相同，IPv4地址分别为：</strong></p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">185.199.109.153</span><br><span class="hljs-number">185.199.110.153</span><br><span class="hljs-number">185.199.111.153</span><br></code></pre></td></tr></table></figure><p><strong>完成之后应该能看到列表中有以下四条这样的记录，除了<code>名称</code>是自己的域名外其他应该都和图中相同</strong></p><figure><img src="https://pic.qinyu.space/image/image-20231204225200173.png"alt="image-20231204225200173" /><figcaption aria-hidden="true">image-20231204225200173</figcaption></figure><p>上述添加的4条ip地址均是GitHub Pages 的 IP 地址，具体可查看【<ahref="https://docs.github.com/zh/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site">https://docs.github.com/zh/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site</a>】</p></li><li><p>点击继续后，cloudflare会要求将我们DNS服务器修改为以下图中所示的的服务器，可以先截个图或者存文档里：</p><figure><img src="https://pic.qinyu.space/image/image-20231204230559749.png"alt="image-20231204230559749" /><figcaption aria-hidden="true">image-20231204230559749</figcaption></figure></li><li><p>点击下方继续后会有一个快速入门指南，里面的配置可以都开启：</p><figure><img src="https://pic.qinyu.space/image/image-20231204233423686.png"alt="image-20231204233423686" /><figcaption aria-hidden="true">image-20231204233423686</figcaption></figure></li></ol><h3 id="修改dns服务器">3.2 修改DNS服务器</h3><p>进入腾讯云控制台，修改的DNS服务器为3.1 第7步中 cloudflare提供的DNS服务器，如下所示：</p><figure><img src="https://pic.qinyu.space/image/image-20231204231234047.png"alt="image-20231204231234047" /><figcaption aria-hidden="true">image-20231204231234047</figcaption></figure><p>DNS服务器更改后生效需要一段时间，少则几分钟，慢则需要几个小时</p><p>过一段时间可以看到 DNS服务器已经修改成功了</p><figure><img src="https://pic.qinyu.space/image/image-20231204232327896.png"alt="image-20231204232327896" /><figcaption aria-hidden="true">image-20231204232327896</figcaption></figure><p>返回 cloudflare，如果看到 <strong>“Cloudflare正在保护您的站点”</strong>说明已经配置成功了：</p><figure><img src="https://pic.qinyu.space/image/image-20231204233838791.png"alt="image-20231204233838791" /><figcaption aria-hidden="true">image-20231204233838791</figcaption></figure><h3 id="设置github-page">3.3 设置Github page</h3><p>进入github.io对应的仓库，进入 <code>Settings</code>：</p><figure><img src="https://pic.qinyu.space/image/image-20231204234453901.png"alt="image-20231204234453901" /><figcaption aria-hidden="true">image-20231204234453901</figcaption></figure><p>进入左栏中的<code>pages</code>，在<code>Custom domain</code>中输入自己的域名，点击<code>save</code>，如果成功会显示下图：</p><figure><img src="https://pic.qinyu.space/image/image-20231204234850740.png"alt="image-20231204234850740" /><figcaption aria-hidden="true">image-20231204234850740</figcaption></figure><p>这样就可以通过域名来访问自己的博客了，还可以在上图中勾选<code>Enforcrs HTTPS</code>，这样网站仅会通过https提供服务。</p><div class="note note-info">            <p>如果如下图显示dns配置不正确，推测可能是使用了cloudflare后，GitHub验证DNS时返回的是cdn服务器的ip地址，而不是在cloudflare上开始配置的4个GitHubpage的ip地址，可以在线dig一下自己的域名验证一下。不过只要网站能通过域名正常访问就没什么问题。</p><p><img src="https://pic.qinyu.space/image/image-20231204235122269.png" style="zoom:33%;" /></p>          </div>]]></content>
    
    
    <categories>
      
      <category>博客搭建</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cdn</tag>
      
      <tag>cloudflare</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性神经网络</title>
    <link href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="深度学习线性神经网络">深度学习——线性神经网络</h2><p>线性回归的输出通常是预测目标的值，softmax回归的输出通常是预测目标的概率分布</p><span id="more"></span><h3 id="线性回归">1. 线性回归</h3><ol type="1"><li><p>问题类型</p><p>线性回归通常用于预测一个连续的目标变量，比如房价与房龄、面积的关系：</p><p><span class="math display">\[\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} +w_{\mathrm{age}} \cdot \mathrm{age} + b.\]</span> 更为一般的会写成： <span class="math display">\[\hat{y} = w_1  x_1 + ... + w_d  x_d + b = \mathbf{w}^\top \mathbf{x} +b.\]</span></p></li><li><p>输出层</p><p>线性回归的的输出就是模型的原始预测值，比如输出的是5000就代表<spanclass="math inline">\(\mathrm{price}\)</span>是5000,一般不会再经过任何变换。</p></li><li><p>损失函数</p><p>线性回归的损失函数通常使用均方误差损失函数（Mean SquaredError，MSE）当样本<span class="math inline">\(i\)</span>的预测值为<spanclass="math inline">\(\hat{y}^{(i)}\)</span>，其相应的真实标签为<spanclass="math inline">\(y^{(i)}\)</span>时，平方误差可以定义为以下公式：<span class="math display">\[l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} -y^{(i)}\right)^2.\]</span> 为了度量模型在整个数据集上的质量，我们需计算在训练集<spanclass="math inline">\(n\)</span>个样本上的损失均值 <spanclass="math display">\[L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b)=\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top\mathbf{x}^{(i)} + b - y^{(i)}\right)^2.\]</span></p></li></ol><h3 id="mathrmsoftmax回归">2. <spanclass="math inline">\(\mathrm{softmax}\)</span>回归</h3><ol type="1"><li><p>问题类型</p><p>用于多类别分类问题，其中每个样本属于且仅属于一个类别。<spanclass="math inline">\(\mathrm{softmax}\)</span>回归通常用于处理分类问题，例如图像分类，手写数字识别等。</p></li><li><p>输出层</p><p>输出通常是模型对<strong>每个类别的预测概率分布</strong>。<spanclass="math inline">\(\mathrm{softmax}\)</span>回归的目标是将输入样本分类到多个类别中的一个，输出是一个概率分布，表示每个类别的概率。</p><figure><img src="https://pic.qinyu.space/image/image-20231120165615396.png"alt="image-20231120165615396" /><figcaption aria-hidden="true">image-20231120165615396</figcaption></figure></li></ol><p><span class="math display">\[\begin{aligned} \mathbf{O} &amp;= \mathbf{X} \mathbf{W} + \mathbf{b}, \\\hat{\mathbf{Y}} &amp; = \mathrm{softmax}(\mathbf{O}). \end{aligned}\]</span></p><p>其中<spanclass="math inline">\(\mathbf{O}\)</span>是未规范化预测，<spanclass="math inline">\(\hat{\mathbf{Y}}\)</span>是预测的概率分布，输出层单元数与类别总数相同。</p><ol start="3" type="1"><li><p>损失函数</p><p>通常使用交叉熵损失函数（Cross-Entropy Loss），这里推荐一篇博客<ahref="https://blog.csdn.net/tsyccnh/article/details/79163834">【一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉】</a>，熵意味着不确定性，在物理学中熵越大，说明系统的混乱程度越大，而在这里交叉熵可以理解为实际的系统（概率分布）与预测的系统（预测的概率分布）之间的差距，因此适用于多类别分类问题。交叉熵损失通过比较模型输出的概率分布与实际标签的概率分布来衡量模型性能。<span class="math display">\[l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.\]</span> <strong><spanclass="math inline">\(y_j\)</span>这里是实际的概率分布，在单分类问题中可以看成是某一类别的独热编码，而<spanclass="math inline">\(\hat{y}_j\)</span>是预测的概率分布，交叉熵是用来衡量两个概率分布之间的相似性或差异性的一种度量</strong>，交叉熵越大说明实际的概率分布和预测的概率分布的越大，通过梯度下降可以降低交叉熵损失函数值。</p><p>举个例子，比如有一个猫、狗、人的<strong>单分类</strong>的图片分类器模型，标签对应的独热编码为：</p><table><thead><tr class="header"><th>猫</th><th>狗</th><th>人</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\([1,0,0]\)</span></td><td><span class="math inline">\([0,1,0]\)</span></td><td><span class="math inline">\([0,0,1]\)</span></td></tr></tbody></table><p>由于是单分类问题，一张图片中只会有猫、狗、人中的某一种，不会出现猫和狗同时出现的情况，因此类别对应的独热编码就可以视为是实际的概率分布，比如有一张图片的标签为<spanclass="math inline">\([1,0,0]\)</span>，则可以确定这张图片实际为猫的概率为1，为狗、人的概率均为0。</p><p>同样是这张图片，经过模型<spanclass="math inline">\(\mathrm{softmax}\)</span>之后预测的概率为<spanclass="math inline">\(\hat{y}_i=[0.5,0.4,0.1]\)</span>，即预测为猫、狗、人的概率分别为0.5、0.4、0.1，现在用交叉熵损失函数计算预测与实际的误差：<span class="math display">\[l= -1 \times \log 0.5-0 \times \log 0.4 - 0 \times \log 0.1 = 0.301\]</span> 再考虑两个极端情况</p><ol type="1"><li><p>预测的概率为<spanclass="math inline">\(\hat{y}_i=[1,0,0]\)</span>，即完全预测正确</p><p>交叉熵为： <span class="math display">\[l= -1 \times \log 1-0 \times \log 0 - 0 \times \log 0 = 0\]</span></p></li><li><p>预测的概率为<spanclass="math inline">\(\hat{y}_i=[0,0.5,0.5]\)</span>，即完全预测错误</p><p>交叉熵为： <span class="math display">\[l= -1 \times \log 0-0 \times \log 0.5 - 0 \times \log 0.5 = +\infty\]</span></p></li></ol><p>可见在完全预测正确的情况下交叉熵最小，完全预测错误的情况下交叉熵最大，即实际的概率分布和预测的概率分布的最大。</p><p>因此交叉熵能很好的作为分类问题的损失函数。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线性回归</tag>
      
      <tag>softmax回归</tag>
      
      <tag>交叉熵</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
