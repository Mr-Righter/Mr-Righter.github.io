<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>线性神经网络</title>
    <link href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="深度学习线性神经网络">深度学习——线性神经网络</h2><p>线性回归的输出通常是预测目标的值，softmax回归的输出通常是预测目标的概率分布</p><span id="more"></span><h3 id="线性回归">1. 线性回归</h3><ol type="1"><li><p>问题类型</p><p>线性回归通常用于预测一个连续的目标变量，比如房价与房龄、面积的关系：</p><p><span class="math display">\[\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} +w_{\mathrm{age}} \cdot \mathrm{age} + b.\]</span> 更为一般的会写成： <span class="math display">\[\hat{y} = w_1  x_1 + ... + w_d  x_d + b = \mathbf{w}^\top \mathbf{x} +b.\]</span></p></li><li><p>输出层</p><p>线性回归的的输出就是模型的原始预测值，比如输出的是5000就代表<spanclass="math inline">\(\mathrm{price}\)</span>是5000,一般不会再经过任何变换。</p></li><li><p>损失函数</p><p>线性回归的损失函数通常使用均方误差损失函数（Mean SquaredError，MSE）当样本<span class="math inline">\(i\)</span>的预测值为<spanclass="math inline">\(\hat{y}^{(i)}\)</span>，其相应的真实标签为<spanclass="math inline">\(y^{(i)}\)</span>时，平方误差可以定义为以下公式：<span class="math display">\[l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} -y^{(i)}\right)^2.\]</span> 为了度量模型在整个数据集上的质量，我们需计算在训练集<spanclass="math inline">\(n\)</span>个样本上的损失均值 <spanclass="math display">\[L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b)=\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top\mathbf{x}^{(i)} + b - y^{(i)}\right)^2.\]</span></p></li></ol><h3 id="mathrmsoftmax回归">2. <spanclass="math inline">\(\mathrm{softmax}\)</span>回归</h3><ol type="1"><li><p>问题类型</p><p>用于多类别分类问题，其中每个样本属于且仅属于一个类别。<spanclass="math inline">\(\mathrm{softmax}\)</span>回归通常用于处理分类问题，例如图像分类，手写数字识别等。</p></li><li><p>输出层</p><p>输出通常是模型对<strong>每个类别的预测概率分布</strong>。<spanclass="math inline">\(\mathrm{softmax}\)</span>回归的目标是将输入样本分类到多个类别中的一个，输出是一个概率分布，表示每个类别的概率。</p><figure><imgsrc="https://raw.githubusercontent.com/Mr-Righter/pic_bed/master/image/image-20231120165615396.png"alt="image-20231120165615396" /><figcaption aria-hidden="true">image-20231120165615396</figcaption></figure></li></ol><p><span class="math display">\[\begin{aligned} \mathbf{O} &amp;= \mathbf{X} \mathbf{W} + \mathbf{b}, \\\hat{\mathbf{Y}} &amp; = \mathrm{softmax}(\mathbf{O}). \end{aligned}\]</span></p><p>其中<spanclass="math inline">\(\mathbf{O}\)</span>是未规范化预测，<spanclass="math inline">\(\hat{\mathbf{Y}}\)</span>是预测的概率分布，输出层单元数与类别总数相同。</p><ol start="3" type="1"><li><p>损失函数</p><p>通常使用交叉熵损失函数（Cross-Entropy Loss），这里推荐一篇博客<ahref="https://blog.csdn.net/tsyccnh/article/details/79163834">【一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉】</a>，熵意味着不确定性，在物理学中熵越大，说明系统的混乱程度越大，而在这里交叉熵可以理解为实际的系统（概率分布）与预测的系统（预测的概率分布）之间的差距。在该损失函数适用于多类别分类问题。交叉熵损失通过比较模型输出的概率分布与实际标签的概率分布来衡量模型性能。<span class="math display">\[l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.\]</span> <strong><spanclass="math inline">\(y_j\)</span>这里是实际的概率分布，在单分类问题中可以看成是某一类别的独热编码，而<spanclass="math inline">\(\hat{y}_j\)</span>是预测的概率分布，交叉熵是用来衡量两个概率分布之间的相似性或差异性的一种度量</strong>，交叉熵越大说明实际的概率分布和预测的概率分布的越大，通过梯度下降可以降低交叉熵损失函数值。</p><p>举个例子，比如有一个猫、狗、人的<strong>单分类</strong>的图片分类器模型，标签对应的独热编码为：</p><table><thead><tr class="header"><th>猫</th><th>狗</th><th>人</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\([1,0,0]\)</span></td><td><span class="math inline">\([0,1,0]\)</span></td><td><span class="math inline">\([0,0,1]\)</span></td></tr></tbody></table><p>由于是单分类问题，一张图片中只会有猫、狗、人中的某一种，不会出现猫和狗同时出现的情况，因此类别对应的独热编码就可以视为是实际的概率分布，比如有一张图片的标签为<spanclass="math inline">\([1,0,0]\)</span>，则可以确定这张图片实际为猫的概率为1，为狗、人的概率均为0。</p><p>同样是这张图片，经过模型<spanclass="math inline">\(\mathrm{softmax}\)</span>之后预测的概率为<spanclass="math inline">\(\hat{y}_i=[0.5,0.4,0.1]\)</span>，即预测为猫、狗、人的概率分别为0.5、0.4、0.1，现在用交叉熵损失函数计算预测与实际的误差：<span class="math display">\[l= -1 \times \log 0.5-0 \times \log 0.4 - 0 \times \log 0.1 = 0.301\]</span> 再考虑两个极端情况</p><ol type="1"><li><p>预测的概率为<spanclass="math inline">\(\hat{y}_i=[1,0,0]\)</span>，即完全预测正确</p><p>交叉熵为： <span class="math display">\[l= -1 \times \log 1-0 \times \log 0 - 0 \times \log 0 = 0\]</span></p></li><li><p>预测的概率为<spanclass="math inline">\(\hat{y}_i=[0,0.5,0.5]\)</span>，即完全预测错误</p><p>交叉熵为： <span class="math display">\[l= -1 \times \log 0-0 \times \log 0.5 - 0 \times \log 0.5 = +\infty\]</span></p></li></ol><p>可见在完全预测正确的情况下交叉熵最小，完全预测错误的情况下交叉熵最大，即实际的概率分布和预测的概率分布的最大。</p><p>因此交叉熵能很好的作为分类问题的损失函数。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线性回归</tag>
      
      <tag>softmax回归</tag>
      
      <tag>交叉熵</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
